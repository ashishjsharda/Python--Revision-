{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRXYFHq9ZFQi",
        "outputId": "ac4da190-9ecf-4f43-ef37-af8c436413f1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Downloading model... This may take a few minutes.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "Testing Ollama in Colab...\n",
            "Question: What is machine learning in simple terms?\n",
            "Answer:  Sure, I'd be happy to help! Machine learning is a type of artificial intelligence where computers learn from data, rather than being explicitly programmed. It involves creating algorithms that can analyze and interpret large amounts of data, identify patterns and make predictions or decisions based on those patterns. Essentially, it's a way for machines to improve their performance over time by learning from experience.\n",
            "\n",
            "\n",
            "Interactive Chat (type 'quit' to stop):\n",
            "AI:  I am doing well, thank you for asking! How can I assist you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install and Run Ollama\n",
        "\n",
        "# Step 1: Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Step 2: Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Start Ollama server\n",
        "process = subprocess.Popen(['ollama', 'serve'],\n",
        "                          stdout=subprocess.PIPE,\n",
        "                          stderr=subprocess.PIPE)\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Step 3: Download a model (this might take a few minutes)\n",
        "print(\"Downloading model... This may take a few minutes.\")\n",
        "!ollama pull phi  # Using phi model as it's smaller and faster to download\n",
        "\n",
        "# Step 4: Chat function\n",
        "def chat_with_ollama(model_name, prompt, host=\"http://localhost:11434\"):\n",
        "    \"\"\"Chat with Ollama running in Colab\"\"\"\n",
        "    url = f\"{host}/api/generate\"\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"No response received\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Step 5: Test it out\n",
        "print(\"Testing Ollama in Colab...\")\n",
        "model = \"phi\"\n",
        "question = \"What is machine learning in simple terms?\"\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "answer = chat_with_ollama(model, question)\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Step 6: Interactive chat (optional)\n",
        "def interactive_chat():\n",
        "    print(\"\\nInteractive Chat (type 'quit' to stop):\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            response = chat_with_ollama(\"phi\", user_input)\n",
        "            print(f\"AI: {response}\")\n",
        "\n",
        "# Uncomment the line below if you want interactive chat\n",
        "interactive_chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}